debug = false
timeout_s = 60
artifact_result_threshold = 500  # Auto-create artifact for playbook results longer than this (# of chars)
max_llm_calls = 50
message_batch_timeout = 0.5    # Rolling timeout for batching agent messages (shorter = more responsive)
message_batch_max_wait = 2.0   # Maximum wait prevents starvation (shorter than meetings since direct msgs are more interactive)
timestamp_granularity = 0  # Timestamp precision: 0=seconds, 1=0.1s, 2=0.01s, 3=milliseconds, -1=10s, -2=100s

[model]
provider = "anthropic"
# name = "claude-sonnet-4-5-20250929"
name = "claude-haiku-4-5-20251001"
temperature = 0.2
max_completion_tokens=15000

[model.execution]
# provider = "xai"
# name = "xai/grok-4-1-fast-non-reasoning"
provider = "gemini"
name = "gemini/gemini-3-flash-preview"

[model.compilation]
provider = "anthropic"
name = "claude-sonnet-4-5-20250929"
temperature = 0.2
max_completion_tokens=15000

[llm_cache]
enabled = true
type = "disk"
path = ".llm_cache"

[langfuse]
enabled = false
